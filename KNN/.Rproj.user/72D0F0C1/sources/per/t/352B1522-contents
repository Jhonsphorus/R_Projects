---
title: "KNN6_260720"
author: "Johnson Adebayo"
date: "7/26/2020"
output: html_document
---
Source: https://www.machinelearningplus.com/machine-learning/caret-package/#33howtoimputemissingvaluesusingpreprocess

# A. Packages loading
```{r}
library(ISLR)
library(skimr)
library(RANN)
library(randomForest)
library(fastAdaboost)
library(gbm)
library(xgboost)
library(caret)
library(caretEnsemble)
library(C50)
library(earth)

```


```{r}
#library(ISLR) # Library that contains the Orange juice dataset (OJ)
#data(OJ)
#dt_juice <- OJ
#View(dt_juice)
```

# B. Data Loading
```{r}
#dtURL <- 'https://raw.githubusercontent.com/selva86/datasets/master/orange_juice_withmissing.csv'

#download.file(url = dtURL, destfile = "orange_juice_withmissing.csv")
orange1 <- read.csv("orange_juice_withmissing.csv", header = TRUE)
#View(orange1)

```

```{r}
str(orange1)


```

```{r}
# Checking for missing values - NA
anyNA(orange1)

sapply(orange1, function(x) { 
  
  sum(is.na(x))
  
  })

```

# C. Data Preparation and Preprocessing


## i. How to split the dataset into training and validation?
```{r}
# Create the training and test datasets
set.seed(100) # To replicate the same values of test and training dataset

# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(orange1$Purchase, p=0.8, list=FALSE)

# Step 2: Create the training  dataset
trainData <- orange1[trainRowNumbers,]

# Step 3: Create the test dataset
testData <- orange1[-trainRowNumbers,]

# Store X and Y for later use.
x = trainData[, 2:18] # Predictor variables for the training
y = trainData$Purchase # Response variable for y

```

```{r}
View(trainData)
```

## ii. Descriptive statistics
```{r}
#library(skimr)
skimmed <- skim_to_wide(trainData)
skimmed[, c(1:5, 9:11, 13, 15)]

```


```{r}
skimmed1 <- skim(trainData)
skimmed1
```

## iii. How to impute missing values using preProcess()?
```{r}
#library(caret) # The package that housed preProcess() function
# Create the knn imputation model on the training data
#preProcess_missingdata_model <- preProcess(trainData, method='bagImpute') # Alternative method
preProcess_missingdata_model <- preProcess(trainData, method='knnImpute')
preProcess_missingdata_model
```


```{r}
# Use the imputation model to predict the values of missing data points
#library(RANN)  # required for knnInpute
trainData <- predict(preProcess_missingdata_model, newdata = trainData)
anyNA(trainData)

```

###Note: The preprocessed traindata obtained has been freed from missing values and normalized.
```{r}
#View(trainData)
```

## iv. How to create One-Hot Encoding (dummy variables)

### Suppose if you have a categorical column as one of the features, it needs to be converted to numeric in order for it to be used by the machine learning algorithms.

```{r}
# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model <- dummyVars(Purchase ~ ., data=trainData)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_mat <- predict(dummies_model, newdata = trainData)

# # Convert to dataframe
trainData <- data.frame(trainData_mat) # Converting the result above to dataframe

# # See the structure of the new dataset
str(trainData)
```


```{r}
#View(trainData)
```


## v. How to preprocess to transform the data (Normalization)

### Type of preprocessing are available in caret?

a. range: Normalize values so it ranges between 0 and 1
b. center: Subtract Mean
c. scale: Divide by standard deviation
d. BoxCox: Remove skewness leading to normality. Values must be > 0
e. YeoJohnson: Like BoxCox, but works for negative values.
f. expoTrans: Exponential transformation, works for negative values.
g. pca: Replace with principal components
h. ica: Replace with independent components
i. spatialSign: Project the data to a unit circle

### For our problem, let’s convert all the numeric variables to range between 0 and 1, by setting method=range in preProcess().
```{r}
preProcess_range_model <- preProcess(trainData, method='range')
trainData <- predict(preProcess_range_model, newdata = trainData)

# Append the Y variable
trainData$Purchase <- y # Adding the response variable (column) back to the trainData
```

### Note: All the predictor now range between 0 and 1.
```{r}
#View(trainData)
# Confirming the range of all the predictors
apply(trainData[, 1:10], 2, FUN=function(x){c('min'=min(x), 'max'=max(x))})

```

# D. visualizing the importance of variables using featurePlot()

### Note: The blue box represents the region where most of the regular data points lied.
### In this case, For a variable to be important, I would expect the means of the two box plots to be significantly different for the 2 classes.
```{r}
#str(trainData)

```


```{r}
featurePlot(x = trainData[, 1:18], y = trainData$Purchase, plot = "box")

```


```{r}
featurePlot(x = trainData[, 1:18], 
            y = trainData$Purchase, 
            plot = "box",
            strip=strip.custom(par.strip.text=list(cex=.7)),
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")))

```

### Density Plot: In this case, For a variable to be important, I would expect the density curves to be significantly different for the 2 classes, both in terms of the height (kurtosis) and placement (skewness).
```{r}
featurePlot(x = trainData[, 1:18], y = trainData$Purchase, plot = "density", strip=strip.custom(par.strip.text=list(cex=.7)),scales = list(x = list(relation="free"), y = list(relation="free")))
```

# E. How to do feature selection using recursive feature elimination (rfe)

### Note: Once rfe() is run, the output shows the accuracy and kappa (and their standard deviation) for the different model sizes we provided. The final selected model subset size is marked with a * in the rightmost Selected column.

From the above output, a model size of 3 with LoyalCH, PriceDiff and StoreID seems to achieve the optimal accuracy.
```{r}
set.seed(100)
options(warn=-1)

subsets <- c(1:5, 10, 15, 18)

ctrl <- rfeControl(functions = rfFuncs,method = "repeatedcv",repeats = 5,verbose = FALSE)

lmProfile <- rfe(x=trainData[, 1:18], y=trainData$Purchase,sizes = subsets,rfeControl = ctrl)

lmProfile

```

# F. Training and Tuning the model

## i. How to train() the model and interpret the results
```{r}
# See available algorithms in caret
modelnames <- paste(names(getModelInfo()), collapse=',  ')

modelnames

```


```{r}
# if you want to know more details like the hyperparameters and if it can be used of regression or classification problem
#modelLookup(algo)
```


```{r}
# Set the seed for reproducibility
set.seed(100)

# Train the model using randomForest and predict on the training data itself.
model_mars = train(Purchase ~ ., data=trainData, method='earth')
fitted <- predict(model_mars)

```


```{r}
model_mars
```

```{r}
plot(model_mars, main="Model Accuracies with MARS")

```

## ii. How to compute variable importance
```{r}
varimp_mars <- varImp(model_mars)
plot(varimp_mars, main="Variable Importance with MARS")

```

## iii. Prepare the test dataset and predict

### Note: Here you do all what you did to the train dataset to the test dataset in the following sequence:

Missing Value imputation –> One-Hot Encoding –> Range Normalization

```{r}
# Step 1: Impute missing values 
testData2 <- predict(preProcess_missingdata_model, testData)  

# Step 2: Create one-hot encodings (dummy variables)
testData3 <- predict(dummies_model, testData2)

#testData3 <- data.frame(testData3) # This is optional

# Step 3: Transform the features to range between 0 and 1
testData4 <- predict(preProcess_range_model, testData3)

# View
head(testData4[, 1:10])
#View(testData4)
```

## iv. Predict on testData
### The test dataset is prepared. Let’s predict the Y.

```{r}
# Predict on testData
predicted <- predict(model_mars, testData4)
head(predicted)

```

### v. Confusion Matrix
```{r}
confusionMatrix(reference = testData$Purchase, data = predicted, mode='everything', positive='MM')
#confusionMatrix(predicted, testData$Purchase, positive='MM' )

```

# G. How to do hyperparameter tuning to optimize the model for better performance

### There are two main ways to do hyperparameter tuning using the train() function:
### - Set the tuneLength
### - Define and set the tuneGrid

## i. Setting up the trainControl() i.e. you are setting the steering that will control your model

```{r}
# Define the training control
fitControl <- trainControl(
    method = 'cv',                   # k-fold cross validation
    number = 5,                      # number of folds
    savePredictions = 'final',       # saves predictions for optimal tuning parameter
    classProbs = T,                  # should class probabilities be returned
    summaryFunction=twoClassSummary  # results summary function
) 
```

```{r}
#fitControl2 <- trainControl(method = 'repeatedcv', number = 5, repeats = 3)
```

## ii. Hyper Parameter Tuning using tuneLength

### Let’s take the train() function we used before, plus, additionally set the tuneLength, trControl and metric.
```{r}
# Step 1: Tune hyper parameters by setting tuneLength
set.seed(100)
model_mars2 = train(Purchase ~ ., data=trainData, method='earth', tuneLength = 5, metric='ROC', trControl = fitControl )
model_mars2
```


```{r}
# Step 2: Predict on testData and Compute the confusion matrix
predicted2 <- predict(model_mars2, testData4)
confusionMatrix(reference = testData$Purchase, data = predicted2, mode='everything', positive='MM')

```

### ii. Hyper Parameter Tuning using tuneGrid
```{r}
# Step 1: Define the tuneGrid
marsGrid <-  expand.grid(nprune = c(2, 4, 6, 8, 10), degree = c(1, 2, 3, 4))

# Step 2: Tune hyper parameters by setting tuneGrid
set.seed(1000)
model_mars3 = train(Purchase ~ ., data=trainData, method='earth', metric='ROC', tuneGrid = marsGrid, trControl = fitControl)
model_mars3


```


```{r}
# Step 3: Predict on testData and Compute the confusion matrix
predicted3 <- predict(model_mars3, testData4)
confusionMatrix(reference = testData$Purchase, data = predicted3, mode='everything', positive='MM')

```

# H. How to evaluate performance of multiple machine learning algorithms

## i. Training Adaboost
```{r}

set.seed(100)

# Train the model using adaboost
model_adaboost = train(Purchase ~ ., data=trainData, method='adaboost', tuneLength=2, trControl = fitControl)
model_adaboost
```


```{r}
pred_adaboost <- predict(model_adaboost, testData4)
confusionMatrix(pred_adaboost, testData$Purchase, positive='MM')

```

## ii. Training Random Forest
```{r}
set.seed(100)

# Train the model using rf
model_rf = train(Purchase ~ ., data=trainData, method='rf', tuneLength=5, trControl = fitControl)
model_rf

```

```{r}
pred_rf <- predict(model_rf, testData4)
confusionMatrix(pred_rf, testData$Purchase, positive='MM')

```

## iii. Training xgBoost Dart
```{r}
set.seed(100)

# Train the model using MARS
model_xgbDART = train(Purchase ~ ., data=trainData, method='xgbDART', tuneLength=5, trControl = fitControl, verbose=F)
model_xgbDART

```



```{r}
pred_xgb <- predict(model_xgbDART, testData4)
confusionMatrix(pred_xgb, testData$Purchase, positive = "MM")

```
## iv. Training SVM

```{r}
set.seed(100)

# Train the model using MARS
model_svmRadial = train(Purchase ~ ., data=trainData, method='svmRadial', tuneLength=15, trControl = fitControl)
model_svmRadial
```

```{r}
# Making prediction with the svm model
pred_svm <- predict(model_svmRadial, testData4)
confusionMatrix(pred_svm, testData$Purchase)

```

## v. Run resamples() to compare the models
```{r}
# Compare model performances using resample()
models_compare <- resamples(list(ADABOOST=model_adaboost, RF=model_rf, XGBDART=model_xgbDART, MARS=model_mars3, SVM=model_svmRadial))

# Summary of the models performances
summary(models_compare)

```

### The xgbDART model appears to be the be best performing model overall because of the high ROC. But if you need a model that predicts the positives better, you might want to consider MARS, given its high sensitivity.
```{r}
# Draw box plots to compare models
scales <- list(x=list(relation="free"), y=list(relation="free"))
bwplot(models_compare, scales=scales)

```

# I. Ensembling the predictions
## i. How to ensemble predictions from multiple models using caretEnsemble
```{r}
#library(caretEnsemble)

# Stacking Algorithms - Run multiple algos in one call.
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)

algorithmList <- c('rf', 'adaboost', 'earth', 'xgbDART', 'svmRadial')

set.seed(100)
models <- caretList(Purchase ~ ., data=trainData, trControl=trainControl, methodList=algorithmList) 
results <- resamples(models)
summary(results)

```

```{r}
# Draw box plots to compare models
scales <- list(x=list(relation="free"), y=list(relation="free"))
bwplot(results, scales=scales)

```


## ii. How to combine the predictions of multiple models to form a final prediction

### This wilkl combine multiple algorithms to build a model and make prediction
```{r}
# Create the trainControl
set.seed(101)
stackControl <- trainControl(method="repeatedcv", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)

# Ensemble the predictions of `models` to form a new combined prediction based on glm
stack.glm <- caretStack(models, method="glm", metric="Accuracy", trControl=stackControl)
print(stack.glm)

```

```{r}
# Predict on testData
stack_predicteds <- predict(stack.glm, newdata=testData4)
head(stack_predicteds)
```

```{r}
confusionMatrix(stack_predicteds, testData$Purchase, positive = 'MM')
```

```{r}

```

```{r}

```

```{r}

```

```{r}

```






























